\documentclass[11pt,t]{beamer}
\mode<presentation> {\usetheme{kuleuven}}

\usepackage{colortbl}
\usepackage{multirow}

\title{9 Months Progress Report}
\author{Louis Onrust}
\institute{CLS, Radboud University \and ESAT-PSI, KU Leuven}

\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}


\usepackage{environ}% Required for \NewEnviron, i.e. to read the whole body of the environment
\makeatletter

\newcounter{acolumn}%  Number of current column
\newlength{\acolumnmaxheight}%   Maximum column height


% `column` replacement to measure height
\newenvironment{@acolumn}[1]{%
    \stepcounter{acolumn}%
    \begin{lrbox}{\@tempboxa}%
    \begin{minipage}{#1}%
}{%
    \end{minipage}
    \end{lrbox}
    \@tempdimc=\dimexpr\ht\@tempboxa+\dp\@tempboxa\relax
    % Save height of this column:
    \expandafter\xdef\csname acolumn@height@\roman{acolumn}\endcsname{\the\@tempdimc}%
    % Save maximum height
    \ifdim\@tempdimc>\acolumnmaxheight
        \global\acolumnmaxheight=\@tempdimc
    \fi
}

% `column` wrapper which sets the height beforehand
\newenvironment{@@acolumn}[1]{%
    \stepcounter{acolumn}%
    % The \autoheight macro contains a \vspace macro with the maximum height minus the natural column height
    \edef\autoheight{\noexpand\vspace*{\dimexpr\acolumnmaxheight-\csname acolumn@height@\roman{acolumn}\endcsname\relax}}%
    % Call original `column`:
    \orig@column{#1}%
}{%
    \endorig@column
}

% Save orignal `column` environment away
\let\orig@column\column
\let\endorig@column\endcolumn

% `columns` variant with automatic height adjustment
\NewEnviron{acolumns}[1][]{%
    % Init vars:
    \setcounter{acolumn}{0}%
    \setlength{\acolumnmaxheight}{0pt}%
    \def\autoheight{\vspace*{0pt}}%
    % Set `column` environment to special measuring environment
    \let\column\@acolumn
    \let\endcolumn\end@acolumn
    \BODY% measure heights
    % Reset counter for second processing round
    \setcounter{acolumn}{0}%
    % Set `column` environment to wrapper
    \let\column\@@acolumn
    \let\endcolumn\end@@acolumn
    % Finally process columns now for real
    \begin{columns}[#1]%
        \BODY
    \end{columns}%
}
\makeatother

\begin{document}

%title page
    \setbeamertemplate{headline}[title_page]
    \setbeamertemplate{footline}[title_page]
    \csname beamer@calculateheadfoot\endcsname %recalculate head and foot dimension
        \begin{frame}
            \titlepage
        \end{frame}
%head and foot for body text    
    \setbeamertemplate{headline}[body]
    \setbeamertemplate{footline}[body]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Outline}
    \vskip 5mm
    {\parbox{.95\textwidth}{\tableofcontents[hideallsubsections]}}
\end{frame}

%\begin{frame}
%    \titlepage
%\end{frame}
%
%\begin{frame}
%    \frametitle{Overview}
%
%    %\tableofcontents
%    \begin{itemize}
%        \item Scope of the project
%        \item Bayesian language models
%        \item Results
%        \item Research plan
%        \item Side projects
%        \item Reflections
%        \item Formalities
%    \end{itemize}
%
%\end{frame}
\section{Scope of the project}

\begin{frame}    
    \frametitle{Scope of the project}

    \begin{block}{Scope}
        \begin{itemize}
            \item Language models
            \item Latent variable models
            \item Domain-dependence of LVLM
            \item Intrinsic \& extrinsic evaluation
        \end{itemize}
    \end{block}

    \begin{block}{Goal}
        \begin{itemize}
            \item Bring back language modelling in Bayesian language modelling    
            \item Improve cross domain language modelling with skipgrams
        \end{itemize}
    \end{block}
\end{frame}

%\begin{frame}
%    \frametitle{Scope of the project}
%
%    \begin{block}{Latent Variable Models}
%    
%    \end{block}
%
%    \begin{block}{Language Models}
%
%    \end{block}
%\end{frame}
\section{Bayesian Language Model}
\begin{frame}
    \frametitle{Bayesian Language Model}

    \begin{itemize}
        \item The goal is to derive the partition underlying the data
        \item But we only have the word counts
    \end{itemize}

    \begin{block}{Clustering}
        \begin{itemize}
            \item Each $n$-gram is a cluster
            \item Each $n$ is a layer
            \item Each history is in a cluster at the $(n-1)$th layer
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Language Model: The Process}
    \begin{block}{Hierarchical Pitman-Yor Chinese Restaurant Process}
        \begin{itemize}
            \item CRP and DPCRP give logarithmic growth
            \item Language manifests typically in power law growth
            \item PYCRP as generalisation of CRP and DPCRP
                \begin{description}
                    \item[CRP] No parameters
                    \item[DPCRP] Concentration parameter $\alpha$
                    \item[PYCRP] Concentration parameter $\alpha$ and discount parameter $\gamma$
                \end{description}
            \item HPYCRP to model inherent hierarchical structure $n$-gram
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Bayesian Language Model: The Implementation}

    \begin{block}{Implementation}
        We use the following software:
        \begin{description}
            \item[cpyp] an existing \CC{} framework on BNP with PYP priors
            \item[colibri] an existing \CC{} pattern model framework 
        \end{description}
    \end{block}

    \begin{block}{Advantages}
        \begin{itemize}
            \item We can now handle many patterns such as $n$-grams, skipgrams and flexgrams
            \item Tresholding patterns on many levels
            \item Efficient storage of patterns
        \end{itemize}
    \end{block}
\end{frame}

\section{Results}
\begin{frame}
    \frametitle{Results: The Setup}

            \begin{block}{Data sets}
                \begin{itemize}
                    \item JRC English
                    \item Google 1 billion words
                    \item EMEA English
                \end{itemize}
            \end{block}
            \begin{block}{Backoff methods}
                \begin{itemize}
                    \item $n$-gram backoff
                    \item Limited recursive backoff
                    \item Full recursive backoff
                \end{itemize}
            \end{block}

             Intrinsic evaluation with perplexity
\end{frame}

\begin{frame}
    \frametitle{Results: An Overview}
    \begin{block}{Summary}
        \begin{itemize}
            \item Within domain evaluation yields best performance
            \item Adding skipgrams increases performance on cross domain evaluation
            \item For generic corpora, limited recursive backoff performs best
            \item Seems to outperform Generalised Language Model
            \item If significant, perhaps not enough for extrinsic evaluation
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Results: Within domain versus cross domain}

    {\small
    \begin{acolumns}[T,totalwidth=\textwidth]
        \begin{column}{0.5\textwidth}
        \begin{block}{Training with only $n$-grams}
            \begin{tabular}{rrrr}
                    & jrc & 1bw  & emea \\ \hline
                jrc & 13  & 1195 & 961 \\
                1bw & 768 & 158  & 945 \\
                emea& 600 & 1143 & 4
            \end{tabular}
        \end{block}
        \end{column}
        \begin{column}{0.5\textwidth}
        \begin{block}{and with skipgrams}
            \begin{tabular}{rrrr}
                    & jrc & 1bw  & emea \\ \hline
                jrc & 13  & 1162 & 939 \\
                1bw & 751 & 162  & 921 \\
                emea& 581 & 1155 & 4
            \end{tabular}
        \end{block}
        \end{column}
    \end{acolumns}

        \begin{block}{Relative differences}
            \begin{tabular}{rrrr}
                    & jrc  & 1bw  & emea \\ \hline
                jrc & 2.0  & \cellcolor{green!25}{-2.8} & \cellcolor{green!25}{-2.3} \\
                1bw & \cellcolor{green!25}{-2.2} & 2.4  & \cellcolor{green!25}{-2.6} \\
                emea& \cellcolor{green!25}{-3.2} & 1.1  & 0.7
            \end{tabular}
        \end{block}
    }
\end{frame}

\begin{frame}
    \frametitle{Results: Effects of different backoff methods}

    {\small
    \begin{acolumns}[T,totalwidth=\textwidth]
        \begin{column}{0.5\textwidth}
                \begin{block}{\hspace{2.55cm}$n$-grams\vphantom{Skipgrams}}
                \begin{tabular}{rrrrr}
                                          &       & jrc & 1bw  & emea \\ \cline{3-5}
                    \multirow{3}{*}{jrc}  & ngram & \cellcolor{green!25}{13}  & 1510 & 1081 \\
                                          & limited& 14  & 1477 & 1122 \\
                                          & full   & 69  & \cellcolor{green!25}{1195} & \cellcolor{green!25}{961}  \\
         &&&& \\
                    \multirow{3}{*}{1bws} & ngram & \cellcolor{green!25}{768} & \cellcolor{green!25}{158}  & \cellcolor{green!25}{946}  \\
                                          & limited& 815 & 185  & 1025 \\
                                          & full   & 801 & 264  & 1039 \\ 
         &&&& \\ 
                    \multirow{3}{*}{emea} & ngram & 769 & 1552 & \cellcolor{green!25}{4}    \\   
                                          & limited& 779 & 1385 & \cellcolor{green!25}{4}    \\
                                          & full   & \cellcolor{green!25}{600} & \cellcolor{green!25}{1143} & 32
                \end{tabular}
            \end{block}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{block}{Skipgrams}
                \begin{tabular}{rrr}
                    jrc & 1bw  & emea \\ \hline
                    \cellcolor{green!25}{13}  & 1843 & 1295 \\
                    \cellcolor{green!25}{13}  & 1542 & 1149 \\
                    65  & \cellcolor{green!25}{1195} & \cellcolor{green!25}{939}  \\
         && \\
                    879 & 163  & 1105 \\
                    \cellcolor{green!25}{751} & \cellcolor{green!25}{162}  & \cellcolor{green!25}{921}  \\
                    768 & 252  & 988  \\ 
         && \\
                    969 & 2089 & \cellcolor{green!25}{4}    \\   
                    838 & 1655 & \cellcolor{green!25}{4}    \\
                    \cellcolor{green!25}{581} & \cellcolor{green!25}{1155} & 32
                \end{tabular}
            \end{block}
        \end{column}
    \end{acolumns}
    }

\end{frame}

\section{Research Plan}
\begin{frame}
    \frametitle{Research Plan}

             Cross domain language modelling with skipgrams

    \begin{block}{Experiments}
        \begin{itemize}
            \item Validate significance by testing multiple languages
            \item Investigate influence skipgrams with qualitative analysis
            \item When we find a more substantial drop in perplexity:
                \begin{itemize}
                    \item Machine translation experiments
                    \item Automated speech recognition experiments
                \end{itemize}
            \item Investigate multi-domain language models
        \end{itemize}
    \end{block}

    \begin{block}{Writing in progress}
        \begin{itemize}
            \item TACL journal paper on our findings
                \begin{itemize}
                    \item ACL, EMNLP, ICASSP, \ldots
                \end{itemize}
            \item Background/Methodology section of PhD thesis
        \end{itemize}
    \end{block}
\end{frame}

\section{Side Projects}
\begin{frame}
    \frametitle{Side Projects}

    \begin{block}{Parsimonious Language Models}
        The goal is to model the differences between corpora
        \begin{itemize}
            \item Only store salient differences:
                \begin{itemize}
                    \item document-specific terms and patterns
                    \item domain-specific terms and patterns
                \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Realistic Motif Detection}
        The goal is to find motifs in folk tales at a sentential level
        \begin{itemize}
            \item Take order of motifs in consideration
            \item Sentences can take any number of motifs
            \item Un-, semi-, and supervised learning
            \item Incorporation of domain and genre knowledge
        \end{itemize}
    \end{block}
\end{frame}

\section{Reflections}
\begin{frame}
    \frametitle{Reflections}

    \begin{block}{Struggling with reproducing results}
        \begin{itemize}
            \item No data or code provisional
            \item Instructions unclear and fuzzy
            \item Fast pacing and non-dedicated research lines
        \end{itemize}
    \end{block}

    \begin{block}{Missed the boat}
        \begin{itemize}
            \item Good ideas, but obviated by other publications
                \begin{itemize}
                    \item HPYLM with $n\rightarrow\infty$: Stochastic Memoiser
                    \item Bayesian PLM
                \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Reflections: pt. 2}
    \begin{block}{}
        Little help from outside, but learned anyway
        \begin{itemize}
            \item A lot of literature, but confusing or contradicting
            \item Still a relative small research community
            \item Good foundation for further work
        \end{itemize}

    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Formalities}

    \begin{block}{Teaching and Supervision}
        \begin{itemize}
            \item Supervision of master students in a competition on sentiment analysis
            \item Supervision of a master student for a task to predict reduction in speech
        \end{itemize}
    \end{block}

    \begin{block}{Training and Education}
        \begin{acolumns}[T,totalwidth=\textwidth]
            \begin{column}{0.5\textwidth}
                \begin{block}{Participated}
                    \begin{itemize}
                        \item Academic writing
                        \item Research methods and methodology
                        \item Applied Bayesian statistics school on Bayesian non-parametrics
                    \end{itemize}
                    \autoheight
                \end{block}
            \end{column}    
            \begin{column}{0.5\textwidth}
                \begin{block}{To participate in}
                    \begin{itemize}
                        \item Mathematical methods
                        \item Presentation skills
                        \item Any relevant event
                    \end{itemize}
                    \autoheight
                \end{block}
            \end{column}    
        \end{acolumns}
    \end{block}
\end{frame}

\end{document}
