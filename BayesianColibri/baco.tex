%\documentclass{beamer}
%\setbeameroption{show notes}
\usetheme[official=true]{ruhuisstijl}
%\usepackage{graphicx}
%\usepackage[english]{babel}
%\usepackage{listings,amsmath}
%\usepackage{algorithm2e}
%\usepackage{graphicx}
%\usepackage{booktabs}
%\usepackage{array}
%\usepackage{ulem}
%\usepackage{palatino}
%\usepackage{color}
%\usepackage{colortbl}
\usepackage{multirow}
%\usepackage{tikz}
%\usetikzlibrary{positioning,shapes.geometric}
%\usepackage{mathptmx}
%\usepackage[scaled=.90]{helvet}
%\usepackage{courier}
%\usepackage[T1]{fontenc}
%\renewcommand{\emph}[1]{\textcolor{rured}{#1}}
%\usepackage{biblatex}
%\newcolumntype{R}{@{\extracolsep{3cm}}r@{\extracolsep{0pt}}}%
%
%\bibliography{baco}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}


\author{Louis Onrust}
\title{$p(\text{conclusions} | \text{Skipping \{*2*\}})$}
\subtitle{Bayesian Language Modelling with Skipgrams}
\date{}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}
\note[itemize]{
}

\begin{frame}{Bayesian Language Modelling with Skipgrams}

\begin{block}{}
    Louis Onrust \\
    Centre for Language Studies, Radboud University \\
    Center for Processing Speech and Images, KU Leuven
\end{block}

\begin{block}{}
    \href{mailto:l.onrust@let.ru.nl}{l.onrust@let.ru.nl} \\
    \href{https://github.com/naiaden}{github.com/naiaden}
\end{block}

\vfill

\begin{block}{\LaTeX{} Beamer template for RU corporate style}
    See my github page for example code:
    \href{https://github.com/naiaden/presentations/tree/master/ruhuisstijl}{github.com/naiaden/presentations/tree/master/ruhuisstijl}
\end{block}

\end{frame}
\note[itemize]{
}

\begin{frame}{Scope of the Project}
    \begin{block}{Scope}
        \begin{itemize}
            \item Language models
            \item Latent variable models
            \item Domain-dependence of LVLM
            \item Intrinsic \& extrinsic evaluation
        \end{itemize}
    \end{block}

    \begin{block}{Goal}
        \begin{itemize}
            \item Bring back language modelling in Bayesian language modelling
            \item Improve cross domain langauge modelling with skipgrams
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Language Model}
    \begin{block}{Traditional method}
        The process:
        \begin{itemize}
            \item Read $n$-gram $p$
            \item Increment frequency of $p$
            \item Repeat, preferably ad infinitum
        \end{itemize}

        $n$-gram probabilities are then determined by their MLE
    \end{block}

    \begin{block}{Smoothed Traditional Language Model}
        What to do when the occurrence count of $p$ is 0?
        \begin{itemize}
            \item Not assign 0 as probability $\rightarrow$ smoothing
            \item Fall back to the $(n-1)$ words of $p$ $\rightarrow$ backoff
        \end{itemize}

        One of the best methods is still Modified Kneser-Ney: backoff and smoothing
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Language Model}
    \begin{block}{Bayesian method}
        \begin{itemize}
            \item Assume texts are generated by some process
            \item Consider the texts to be a sample from the process
            \item Infer underlying process
        \end{itemize}
    \end{block}

    \begin{block}{Bayesian Language Model}
        \begin{itemize}
            \item Each $n$-gram is a cluster
            \item Each $n$ is a layer
            \item Each history is in a cluster at the $(n-1)$th layer
        \end{itemize}
    \end{block}

    \begin{block}{Chinese Restaurant Process}
        \begin{itemize}
            \item Clusters are tables, $n$-grams tokens are customers
            \item Initially tokens seat at the same table
            \item In the inference step, customers get to choose a new identity
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Bayesian Language Model}
    \begin{block}{Chinese Restaurant Process: Inference}
        When $n$ are in the restaurant, people sit a table $t$ with probability
        \begin{itemize}
            \item $\frac{1}{n}$ with another $n$-gram $p$, or $\frac{|t|}{n}$ at the same table as $p$
            \item $\frac{1}{n}$ at a new table
        \end{itemize}
        The number of tables grows logarithmically
    \end{block}

    \begin{block}{Hierarchical Pitman-Yor Chinese Restaurant Process}
        \begin{itemize}
            \item CRP and DPCRP give logarithmic growth
            \item Language manifests typically in power law growth
            \item PYCRP as generalisation of CRP and DPCRP
            \begin{description}
                \item[CRP] No parameters
                \item[DPCRP] Concentration parameter $\alpha$
                \item[PYCRP] Concentration parameter $\alpha$ and discount parameter $\gamma$
            \end{description}
            \item HPYCRP to model inherent hierarchical structure $n$-gram
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Bayesian Language Model: The Implementation}
    \begin{block}{Motivation}
        Existing Bayesian language models\ldots
        \begin{itemize}
            \item are merely an algorithmic showcase without real language modelling aspirations
            \item cannot handle really big data sets
        \end{itemize}
    \end{block}

    \begin{block}{Implementation}
        We use the following software:
        \begin{description}
            \item[cpyp] an existing \CC{} framework on BNP with PYP priors
            \item[colibri] an existing \CC{} framework for pattern modelling
        \end{description}
    \end{block}

    \begin{block}{Advantages}
        \begin{itemize}
            \item We can now handle many patterns such as $n$-grams, skipgrams, and flexgrams
            \item Tresholding patterns on many levels
            %\item Efficient storage of patterns
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: The Setup}
    \begin{block}{Data Sets}
        \begin{itemize}
            \item JRC English
            \item Google 1 billion words
            \item EMEA English
        \end{itemize}
    \end{block}

    \begin{block}{Backoff Methods}
        \begin{description}
            \item[$n$-gram] full recursive backoff to shorter $n$-grams
            \item[Limited] recursive backoff to all patterns $\leq n$ until match
            \item[Full] recursive backoff to all patterns $\leq n$
        \end{description}
    \end{block}

    \begin{block}{Evaluation Measure}
        \begin{itemize}
            \item Intrinsic evaluation with perplexity
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: An Overview}
    \begin{block}{Summary}
        \begin{itemize}
            \item Within domain evaluation yields best performance
            \item Adding skipgrams increases performance on cross domain evaluation
            \item For generic corpora, limited recursive backoff performs best
            \item Seems to outperform Generalised Language Model
            \item If significant, perhaps not enough for extrinsic evaluation
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: Domains and Patterns}

    \begin{block}{Observations}
        \begin{description}
            \item[domains] Within domain evaluation yields best performance
            \item[patterns] Adding skipgrams increases performance on cross domain evaluation
        \end{description}
    \end{block}
    \vspace{-1em}
    {\small
        \begin{columns}[T,totalwidth=\textwidth]
            \begin{column}{0.5\textwidth}
                \begin{block}{Training with only $n$-grams}
                    \rowcolors{1}{ruhuisstijlrood!12}{ruhuisstijlrood!25}
                    \begin{tabular}{rrrr}
                            & jrc & 1bw  & emea \\ \hline
                        jrc & 13  & 1195 & 961 \\
                        1bw & 768 & 158  & 945 \\
                        emea& 600 & 1143 & 4
                    \end{tabular}
                \end{block}
            \end{column}
            \begin{column}{0.5\textwidth}
                \begin{block}{and with skipgrams}
                    \rowcolors{1}{ruhuisstijlrood!12}{ruhuisstijlrood!25}
                    \begin{tabular}{rrrr}
                            & jrc & 1bw  & emea \\ \hline
                        jrc & 13  & 1162 & 939 \\
                        1bw & 751 & 162  & 921 \\
                        emea& 581 & 1155 & 4
                    \end{tabular}
                \end{block}
            \end{column}
        \end{columns}
        \hspace{1em}
        \begin{block}{Relative differences}
            \rowcolors{1}{ruhuisstijlrood!12}{ruhuisstijlrood!25}
            \begin{tabular}{rrrr}
                    & jrc & 1bw & emea \\ \hline
                jrc & 2.0 & \cellcolor{green!25}{-2.8} & \cellcolor{green!25}{-2.3} \\
                1bw & \cellcolor{green!25}{-2.2} & 2.4 & \cellcolor{green!25}{-2.6} \\
                emea& \cellcolor{green!25}{-3.2} & 1.1 & 0.7
            \end{tabular}
        \end{block}
    }
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: Effect of Different Backoff Methods}
    \begin{block}{Observations}
        \begin{description}
            \item[backoff] For generic corpora, limited recursive backoff performs best
        \end{description}
    \end{block}
    \vspace{-1em}
    {\small
        \begin{columns}[T,totalwidth=\textwidth]
            \begin{column}{0.5\textwidth}
                \begin{block}{\hspace{2.55cm}$n$-grams\vphantom{Skipgrams}}
                    \rowcolors{1}{ruhuisstijlrood!12}{ruhuisstijlrood!25}
                    \begin{tabular}{rrrrr}
                        & & jrc & 1bw & emea \\ \cline{3-5}
                        \multirow{3}{*}{jrc} & ngram & \cellcolor{green!25}{13} & 1510 & 1081 \\
                        & limited& 14 & 1477 & 1122 \\
                        & full & 69 & \cellcolor{green!25}{1195} & \cellcolor{green!25}{961} \\
                        &&&& \\
                        \multirow{3}{*}{1bws} & ngram & \cellcolor{green!25}{768} & \cellcolor{green!25}{158} & \cellcolor{green!25}{946} \\
                        & limited& 815 & 185 & 1025 \\
                        & full & 801 & 264 & 1039 \\
                        &&&& \\
                        \multirow{3}{*}{emea} & ngram & 769 & 1552 & \cellcolor{green!25}{4} \\
                        & limited& 779 & 1385 & \cellcolor{green!25}{4} \\
                        & full & \cellcolor{green!25}{600} & \cellcolor{green!25}{1143} & 32
                    \end{tabular}
                \end{block}
            \end{column}
            \begin{column}{0.5\textwidth}
                \begin{block}{Skipgrams}
                    \rowcolors{1}{ruhuisstijlrood!12}{ruhuisstijlrood!25}
                    \begin{tabular}{rrr}
                        jrc & 1bw & emea \\ \hline
                        \cellcolor{green!25}{13} & 1843 & 1295 \\
                        \cellcolor{green!25}{13} & 1542 & 1149 \\
                        65 & \cellcolor{green!25}{1195} & \cellcolor{green!25}{939} \\
                        && \\
                        879 & 163 & 1105 \\
                        \cellcolor{green!25}{751} & \cellcolor{green!25}{162} & \cellcolor{green!25}{921} \\
                        768 & 252 & 988 \\
                        && \\
                        969 & 2089 & \cellcolor{green!25}{4} \\
                        838 & 1655 & \cellcolor{green!25}{4} \\
                        \cellcolor{green!25}{581} & \cellcolor{green!25}{1155} & 32
                    \end{tabular}
                \end{block}
            \end{column}
        \end{columns}
    }
\end{frame}
\note[itemize]{
}

\begin{frame}{Future Work}
    \begin{block}{Experiments}
        \begin{itemize}
            \item Validate significance by testing on multiple languages
            \item Investigate influence skipgrams with qualitative analysis
            \item When we find a more substantial drop in perplexity:
                \begin{itemize}
                    \item Machine translation experiments
                    \item Automated speech recognition experiments
                \end{itemize}
            \item Investigate multi-domain language models (DHPYPLM)
            \item Generalise skipgrams to flexgrams
            \item \ldots
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{tussenpagina}{}
    

    \begin{figure}
        \includegraphics[width=0.95\textwidth]{images/dilbert.png}
        \vspace{-0.2cm}
        \caption{\href{http://dilbert.com/strips/comic/2009-09-17/}{\textcolor{black}{http://dilbert.com/strips/comic/2009-09-17}}}
    \end{figure}
%        \href{http://dilbert.com/strips/comic/2009-09-17/}{http://dilbert.com/strips/comic/2009-09-17/}
\end{tussenpagina}

\end{document}
