\documentclass{beamer}
\usetheme{rured}
\setbeamertemplate{navigation symbols}{}
\usepackage{graphicx}
\setbeameroption{show notes}
\usepackage[english]{babel}
\usepackage{listings,amsmath}
\usepackage{algorithm2e}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{palatino}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
\usepackage[T1]{fontenc}
\renewcommand{\emph}[1]{\textcolor{rured}{#1}}
\usepackage{biblatex}

\bibliography{baco}

\author{Louis Onrust}
\title{$p(\text{conclusions} | \text{Skipping \{*2*\}})$}
\subtitle{Bayesian Language Modelling with Skipgrams}
\date{}
\begin{document}

\begin{frame}
	\titlepage
\end{frame}
\note[itemize]{
    \item Gentle introduction to show what I've been working with
    \item If you are familiar with skipgrams, you know that the title can mean either
    \begin{itemize}
        \item Skipping to some conclusions
        \item Skipping apple pie conclusions
        \item Skipping without any conclusions
    \end{itemize}
    \item Let's see what's on the menu today
}

\begin{frame}
    \frametitle{The topics for today}
    \begin{itemize}
        \item Bayesian non-parametrics
        \item Language modelling
        \item Chinese restaurants
        \item Poor results
        \item Skipgrams
        \item The future
    \end{itemize}
\end{frame}
\note[itemize]{
    \item It seems like we've got a very varied menu
    \item It's in no particular order
    \item But we will take them all anyway
}

\begin{frame}
    \frametitle{Bayesian Non-Parametrics}

    \begin{block}{Motivation for BNP}
        \begin{itemize}
            \item As the data grows, more patterns emerge:
            \begin{itemize}
                \item this implies a growing, unbounded number of degrees of freedom;
                \item risk of overfitting.
            \end{itemize}
        \end{itemize}   
    \end{block}

    \begin{block}{Non-parametric with parameters}
        \begin{itemize}
            \item Non-parametric does not mean there are no parameters:
            \begin{itemize}
                \item instead, the number of parameters is not fixed.
                \item ``Allows an infinite number of parameters.''
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
    \item Basic motivation is the idea that in many stat.inf.prob.\ we expect that structures or pattern continue to emerge as data accrue, perhaps ad infinitum
    \item We want a modelling framework that supplies a growing, unbounded number of d.o.f.\ to the data analysis
    \item However, if we allow d.o.f.\ to accrue too quickly, we risk finding structures that are statistical artifacts: that is, we overfit.
    \item Overfitting is a serious problem, and it motivates the Bayesian aspect of non-parametrics
    \item Bayesian methods are by no means immune to overfitting, but provide a natural resilience

    \item NP does not mean there are no parameters
    \item not parametric, the numbers of parameters is not fixed once and for all
    \item So it's not opposed to parameters, to the contrary: the framework can be viewed as allowing an infinite number of parameters
}

\begin{frame}[label=dirichletprocess]
    \frametitle{Clustering and the First Formulae}

    \begin{block}{Dirichlet Process}
        \begin{itemize}
            \item Centrepiece of BNP: Introduced by Ferguson\footfullcite{Ferguson73}. 
            \item Infinite-dimensional analog of the \hyperlink{dirichletsupplemental}{Dirichlet distribution}. 
        \end{itemize}
    \end{block}

    \begin{block}{Clustering}
        \begin{itemize}
            \item Each data point $x_i$ is assigned to one of $K$ clusters
            \begin{itemize}
                \item with probability $w_k$, for $k=1,2,\ldots,K$ and $\sum_{k=1}^Kw_k=1$ and a Dirichlet prior placed on the probabilities $\{w_k\}$.
            \end{itemize}
            \item Treat clustering problem as inferring partitions
            \begin{itemize}
                \item by placing probability distributions on partitions;
                \item Chinese Restaurant Process
            \end{itemize}
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
    \item Now we laid some ground for our BNP framework
    \item Dirichlet distribution plays an important role in Bay.stat.\ as conjugate distribution to the multinomial and categorical.
    
    \item A standard Bay.\ model for clustering involves assuming each data point is assigned to one of $K$ clusters
    \item The probability of being in cluster $k$ is $w_k$ for each cluster, and the probability vector is normalised.
    \item We place a dirichlet prior on the prob vector $\{w_k\}$
    \item As an infinite-dimensional analog, you would expect to take $K$ to infinity to derive at the DP
    \item But that is not the BNP approach: instead we take a combined combinatorial and statistical approach to the clustering problems
    \item where we want to infer the partition underlying the data
    \item From a Bayesian p.o.v., this requires placing a prob dist on partitions: in particular, the CRP
    \item KMeans
}

\begin{frame}
    \frametitle{Clusteling with a Chinese Lestaulant Plocess}
    
    \begin{block}{Partitions and Clusters}
        \begin{itemize}
            \item A partition of $N$ points is denoted as $\pi_{[N]}$
            \begin{itemize}
                \item $\pi_{[10]} = \{\{3,5\},\{1,2,9,10\},\{4,6,7\},\{8\}\}$
                \item $\pi_{[N]}$ is a set of subsets, where subsets are clusters
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Chinese Restaurant Process}
        \begin{itemize}
            \item CRP is a probability distribution on partitions.
            \item Restaurant metaphor: points are customers, clusters are tables.
            \item Sequential process: each point at a time is added to an existing set of clusters.
            \begin{itemize}
                \item The first customer is seated alone;
                \item Each subsequent customer is either:
                \begin{itemize}
                    \item seated at one of the already occupied tables, or
                    \item starting a new table.
                \end{itemize}
            \end{itemize}   
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
    \item Bayesian approach is model-based, which means that we first have to specify a generative model
    \item Let's denote a partition of $N$ points as $\pi_{[N]}$
    \item this is a set of $N$ points, where each point belongs to exactly one subset
    \item subsets are clusters, and the ordering of clusters and points within clusters is arbitrary

    \item CRP is a probability distribution over partitions
    \item distribution is built up in a sequential manner: one point at a time is added to an existing set of clusters
    \item metaphore of a restaurant

    \item sits at a already occupied table with probability proportional to the number of customers sitting at that table;
    \item starts a new table with probability proportional to a fixed constant.
}

\begin{frame}
    \[
        P(\text{customer }n+1\text{ joins table }c|\pi_{[n]}) = 
          \begin{cases}
           \frac{|c|}{\alpha+n} & \text{if } c \in \pi_{[n]}, \\
           \frac{\alpha}{\alpha+n}       & \text{otherwise.}
          \end{cases}
    \]
    
    The seating pattern after $N$ customers \hyperlink{crpsupplemental}{defines} a set of clusters: 
    \[
        \pi_{[N]} \sim \operatorname{CRP}(\alpha, N)
    \]

    \begin{block}{Example for $\{\{1,2,5\},\{3,4\},\{6\}\}$}
        \begin{center}
            \begin{tikzpicture}
                \node[draw, circle] (t1) {};
                \node[draw, circle, right=of t1] (t2) {};
                \node[draw, circle, right=of t2] (t3) {};
                \node[circle, right=of t3] (tdots) {$\ldots$};
                \node[draw, circle, right=of tdots] (t4) {};

                \onslide<2->{\node[node distance=0.1cm,above right=of t1] (c1) {1};}
                \onslide<4->{\node[node distance=0.1cm,above left=of t1] (c2) {2};}
                \onslide<6->{\node[node distance=0.1cm,above right=of t2] (c3) {3};}
                \onslide<8->{\node[node distance=0.1cm,above left=of t2] (c4) {4};}
                \onslide<10->{\node[node distance=0.1cm,below right=of t1] (c5) {5};}
                \onslide<12->{\node[node distance=0.1cm,below right=of t3] (c6) {6};}
            \end{tikzpicture}
        \end{center}            

        $\uncover<3->{P=\frac{\alpha}{\alpha}\uncover<5->{\frac{1}{\alpha+1}\uncover<7->{\frac{\alpha}{\alpha+2}\uncover<9->{\frac{\alpha}{\alpha+3}\uncover<11->{\frac{2}{\alpha+4}\uncover<13->{\frac{\alpha}{\alpha+5}$}}}}}} \uncover<14->{$\rightarrow P(\pi_{[N]}) = \frac{\alpha^K}{\alpha^{(N)}}\prod_{c\in\pi_{[N]}}(|c|-1)!$}
    \end{block}
\end{frame}
\note[itemize]{
    \item So, a new customer joins a table $c$, if $c$ is already a cluster, with prob prop to the number of customers already sitting at that table
    \item otherwise starts a new table, with a prob prop to fixed constant $\alpha$
    \item The seating pattern after $N$ customers thus defines a set of clusters

    \item Now for an example, imagine an infinite number of unlabelled tables
    \item when a customer starts a new table, one of the unlabelled tables is chosen arbitrarily
    \item So after $N$ customers have arrived, their seating pattern defines a set of clusters, and thus a partition.
    \item For each value of $N$, we have a different CRP, and thus a different distribution, making the CRP a family of distributions

    \item The order in which the customers come in does not matter, the CRP has the property of exchangeability. I will not proof it here, but after an example, you can see why, and we can reconstruct the top formula in retrospect.
}

\begin{frame}
    \frametitle{Two-Parameter Poisson-Dirichlet Processes}

    \begin{block}{Why Pitman-Yor Processes?}
        \begin{itemize}
            \item DP generates an infinite number of atoms, with a relatively slow rate.
            \item Many real-world phenomena have a power-law growth.
            \item The DP cannot generate such power-laws, hence we use PYP.
        \end{itemize}
    \end{block}

    \begin{block}{PYP as generalisation of the CRP}
        \begin{itemize}
            \item With DP the rate for selecting a new table is $\frac{\alpha}{\alpha+N}$, and choosing an occupied table goes to $\frac{N}{\alpha+N}$:
            \begin{itemize}
                \item $\sum_{n=1}^N\frac{\alpha}{\alpha+n}\asymp\log(N)$
            \end{itemize}
            \item PYP allows $\alpha$ to grow, with the rate of a discount parameter $\sigma$
            \item PYP reduces to DP with $\sigma=0$.
        \end{itemize}
    \end{block}
    
\end{frame}
\note[itemize]{
    \item Now to the heart of the presentation
    \item So the DP generates an infinite number of atoms, which is nice, but does so at a relatively slow rate
    \item We know of many real-world phenomena that do not match this rate, but follow a power-law growth
    \item The DP does not seem to be the tool we want?

    \item So we introduce yet another generalisation: the Pitman-Yor process
    \item The rate of atoms (tables) is asymptotically growing with the log of customers
    \item The latter quickly dominates the former, and few new tables emerge as N becomes large;
    \item We generalise the DP by adding a dicscount parameter $\sigma$
}

\begin{frame}
    \frametitle{PYCRP}

    \[
        P(\text{customer }n+1\text{ joins table }c|\pi_{[n]}) = 
          \begin{cases}
           \frac{|c|-\sigma}{\alpha+n} & \text{if } c \in \pi_{[n]}, \\
           \frac{\alpha+\sigma K_n}{\alpha+n}       & \text{otherwise.}
          \end{cases}
    \]
    \begin{itemize}
        \item The probability of joining an existing table is reduced by an amount proportional to $\sigma$ relative to the CRP.
        \item Reductions are added to the probability of starting a new table.
    \end{itemize}

    The seating pattern after $N$ customers defines a set of clusters: $\pi_{[N]}\sim\operatorname{PYP}(\alpha,\sigma,N)$
    \[
        P(\pi_{[N]}) = \frac{\alpha(\alpha+\sigma)\cdots(\alpha+\sigma(K_N-1))}{\alpha^{(N)}}\prod_{c\in\pi_{[N]}}(1-\sigma)(2-\sigma)\cdots(|c|-1-\sigma).
    \]
\end{frame}
\note[itemize]{
    \item Although discounts in the restaurant are generally nice, in this case it means that you act as if there are less people on the table
    \item This is especially noticeable in the beginning, when $|c|$ is small
    \item all the discounts you saved by not joining a table, are used for the new table

    \item The seating pattern is thus defined as such
    \item and again we have exchangeability
}

\begin{frame}
    \frametitle{Finally, Language Models}
%    We use $n$-gram LM which use the conditional distributions of each word given a context of $n-1$ words: \[ P(\mathbf{w})\aprox\prod_{i}P(w_i|w_{i-n+1}^{i-1}) \]

    \begin{block}{Unigram LM with PYP}
        $W$ is a fixed vocabulary of $V$ words. For each $w\in W$ let $G(w)$ be the probability of $w$, and $G=[G(w)]_{w\in W}$ the vector of word probabilities. 
\[ G \sim \operatorname{PYP}(\alpha, \sigma, G_0). \] 
    \end{block}

    \begin{block}{Inference for our Unigram LM}
        \begin{itemize}
            \item Training data $\mathcal{D}$ consists of occurrence counts $c_{w}$.
            \item We are interested in the posterior distribution $P(G,\Theta|\mathcal{D}) = P(G,\Theta,\mathcal{D})/P(\mathcal{D})$.
            \begin{itemize}
                \item The CRP marginalises out $G$, replacing it with the seating arrangement $S$
                \item The new posterior is then: $P(S,\Theta|\mathcal{D})=P(S,\Theta,\mathcal{D})/P(\mathcal{D})$.
                \item Predictive probability: $p(w|\mathcal{D})=\int P(w|S,\Theta)P(S,\Theta|\mathcal{D})\text{d}(S,\sigma)$.
                \begin{itemize}
                    \item $P(w|S,\Theta) = \frac{c_w-\sigma t_w}{\alpha+c_\cdot} + \frac{\alpha+\sigma t_\cdot}{\alpha+c_\cdot}$.
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{block}

%    \begin{block}{The Generative Strategy to produce $x_1,x_2,\ldots$}
%        \begin{itemize}
%            \item Notice that we can treat $G$ and $G_0$ as distributions over $W$;
%            \item Let $x1,x2,\ldots$ be a sequence over words drawn i.i.d. from $G$.
%            \item We marginalise over $G$ by relating $x_1,x_2,\ldots$ to i.i.d. draws $y_1,y_2,\ldots$ from $G_0$.
%            \item $x_1$ is assigned the value of $y_1$, and $t$ the current number of draws
%            \item $c_k$ the number of words assigned the value of draw $y_k$ and $c_\cdot=\sum_{k=1}^tc_k$ the number of draws from $G$.
%            \item For each subsequent word $x_{c_\cdot+1}$, we assign it the value of a previous draw $y_k$ with probability $\frac{c_k-\alpha}{\sigma+c_\cdot}$, or assign it the value of a new draw from $G_0$ with probability $\frac{\sigma+\alpha t}{\sigma+c_\cdot}$.
%        \end{itemize}
%    \end{block}

\end{frame}
\note[itemize]{
    \item Notice that first we had $\operatorname{PYP}(\alpha, \sigma, N)$, now we have a distribution over some probability space $\Theta$, which are the words
    \item $G_0$ being the uniform prior: $G_0(w) = \frac{1}{V}$
    \item So now we can put a PYP prior on G

    \item Although we can play two games, a generative game and an inference game, the second is more closely related to our idea of query likelihood
    \item The seating arrangment is the partition we saw earlier, with customers sitting at a table with the dish being the word, and the cardinality of $c$ is the count
    \item tough integral, which typically for Bayesian stuff we cannot compute (because of being an infinite sum)
    \item our focus is on $P(w|S,\Theta)$ with a computation that must look similar to the ones we saw earlier for the PYP.
}

\begin{frame}
    \frametitle{Hierarchical PYP}

    \begin{block}{HPYLM}
        Given a context $\mathbf{u}$, let $G_{\mathbf{u}}(w)$ be the probability of the current word taking on value $w$. 
        The HPYP has a prior for $G_{\mathbf{u}} \sim \operatorname{PYP}(\alpha_{|\mathbf{u}|},\sigma_{|\mathbf{u}|},G_{\pi(\mathbf{u})})$, with $\pi(\mathbf{u})$ being the suffix of $\mathbf{u}$ of all but the first word. 
        $G_{\pi(\mathbf{u})}$ is also unknown, so we place a recursive prior over it, with parameters $\alpha_{|\pi(\mathbf{u})|}, \sigma_{|\pi(\mathbf{u})|}$ 
        and mean vector $G_{\pi(\pi(\mathbf{u}))}$; $G_\varnothing$ being the empty context, which is the same as for the unigram LM.

        Interpolated Kneser-Ney can be considered to be an approximate inference of HPYLM.
    \end{block}

    \begin{block}{Chinese Restaurant Franchise}
        \begin{itemize}
            \item A Chinese Restaurant Franchise consists of Chinese Restaurants
            \item There is a global menu with an unbounded number of dishes
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Adding Bayesiawesomeness to Colibri}
    
    Existing Bayesian language models\ldots
        \begin{itemize}
            \item are merely an algorithmic showcase without real LM aspirations;
            \item cannot handle really big datasets.
        \end{itemize}

    \begin{block}{Colibri}
        \begin{itemize}
            \item C++ and Python library for basic linguistic constructions
            \item Generates $n$-grams, skipgrams, and flexgrams
            \item \url{http://proycon.github.io/colibri-core/}
        \end{itemize}
    \end{block}
    \vspace{-0.25cm}
    \begin{block}{Bayesian Colibri}
        \begin{itemize}
            \item We extend the C++ library for NBP with PYP (\url{https://github.com/redpony/cpyp})
            \item BaCo is available at \url{https://github.com/naiaden/cococpyp}
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{From $n$-gram to $Y$-gram}

    We want to beat a MKN approach with skipgrams\footfullcite{Pickhardt14}.

    here some numbers from the grams in different corpora

\end{frame}

\begin{frame}
    \frametitle{Future work}

    DHPYPLM
\end{frame}

\appendix
\begin{frame}[label=dirichletsupplemental]
    \frametitle{Dirichlet Distribution}

    \begin{block}{Informal introduction\footfullcite{Frigyk2010}}
        \begin{itemize}
            \item Dirichlet distribution can model the randomness of pmfs
            \begin{itemize}
                \item We have a dictionary of $k$ possible words
                \item Each document can be represented by a pmf of length $k$ by normalising the empirical frequency of its words
                \item A group of documents produces a collections of pmfs, and we use the Dirichlet distribution to capture the variability
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{block}{Formal introduction}
        Let $Q = [Q_1, Q_2,\ldots,Q_k]$ be a random pmf with $k$ components, that is $Q_i \geq 0$ for $i=1,2,\ldots,k$ and $\sum_{i=1}^k Q_i = 1$
        \[ f(q;\boldsymbol\alpha) = \frac{\Gamma(\alpha_0)}{\prod_{i=1}^k\Gamma(\alpha_i)}\prod_{i=1}^kq_i^{\alpha_i-1}. \]
    \end{block}
Back to the \hyperlink{dirichletprocess}{Dirichlet Process}.
\end{frame}
\note[itemize]{
    \item Example of a pmf is an ordinary six-sided die
    \item To sample from it you roll the die and produce a number one to six
    \item Real dice are not exactly uniformly weighted (physics, manufactoring, \&c.)
    \item A bag of 100 real dice is an example of a random pmf
    \item To sample from it you put your hand in the bag and draw out a die, you draw a pmf
    \item Dice from 100 years ago will likely have probs that deviate wildly from uniform, new sota dice Las Vegas casinos may have barely perceptible imperfections
    \item The Dirichlet distribution can model the randomness of pmfs

    \item Dirichlet distribution is particularly useful in modelling word distributions
    \item $q$ is a pmf, and here a realisation of a random pmf $Q\sim\operatorname{Dir}(\alpha)$
    \item $\boldsymbol\alpha$ is the parameter of the Dirichlet distribution, $\boldsymbol\alpha=[1,1,\ldots,1]$ is uniform over the simplex

    
}

\begin{frame}[label=crpsupplemental]
    \frametitle{Note on exchangeability and customer seating}

    \begin{block}{de Finetti's theorem}
        \begin{itemize}
            \item The therom states why exchangeable observation are conditionally independent given some latent variable
            \begin{itemize}
                \item Assume we use the following conditional with $G$ marginalised out:
                \[ \theta_{n+1}|\theta_1,\ldots,\theta_n\sim\frac{1}{\alpha+n}\left(\alpha H+\sum_{i=1}^n\delta_{\theta_i}\right) \]
                \item For every $n$, the probability of generating $\theta_1,\ldots,\theta_n$ is equal to the probability of drawing them in any alternative order.
                \item More precisely, given any permutation $\sigma$ on $1,\ldots,n$ we have        
    \[ P(\theta_1,\ldots,\theta_n) = P(\theta_{\sigma(1)},\ldots,\theta_{\sigma(n)}) \]
            \end{itemize}
            \item So we have a exchangeable sequence of customers, and while the customers themselves are not independent and identically distributed
        \end{itemize}
    \end{block}



    $P(\text{customer }n+1\text{ joins table }c|\pi_{[n]}) = \frac{P(\pi'_{[n+1]}}{P(\pi_{[n]}}=\frac{\alpha^{(n)}}{\alpha^{(n+1)}}|c| = \frac{|c|}{\alpha+n}$
    
    $P(\text{customer }n+1\text{ starts new table }c|\pi_{[n]})=\frac{P(\pi'_{[n+1]})}{P(\pi_{[n]})}=\frac{\alpha^{K+1}}{\alpha^{(n+1)}}\frac{\alpha^{(n)}}{\alpha^K} = \frac{\alpha}{\alpha+n}$
   
    Back to the \hyperlink{crpexample}{CRP example}. 
\end{frame}

\end{document}
