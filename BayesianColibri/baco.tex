\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabu}

\newcommand{\semitransp}[2][35]{\color{fg!#1}#2}

\colorlet{tableheadcolor}{ruhuisstijlrood} % Table header colour = 25% gray
\newcommand{\headcol}{\rowcolor{tableheadcolor}} %he


\colorlet{tablerowcolorodd}{ruhuisstijlrood!10} % Table row separator colour = 10% gray
\newcommand{\rowcolodd}{\rowcolor{tablerowcolorodd}} %

\colorlet{tablerowcoloreven}{ruhuisstijlrood!25} % Table row separator colour = 10% gray
\newcommand{\rowcoleven}{\rowcolor{tablerowcoloreven}} %



% Command \topline consists of a (slightly modified) \toprule followed by a \heavyrule rule of colour tableheadcolor (hence, 2 separate rules)
\newcommand{\topline}{\arrayrulecolor{black}\specialrule{0.1em}{\abovetopsep}{0.5pt}%
            \arrayrulecolor{tableheadcolor}\specialrule{\belowrulesep}{0pt}{-3pt}%
            \arrayrulecolor{black}
            }
    % Command \midline consists of 3 rules (top colour tableheadcolor, middle colour black, bottom colour white)
\newcommand{\midline}{\arrayrulecolor{tableheadcolor}\specialrule{\aboverulesep}{-1pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{-3pt}%
            \arrayrulecolor{black}
            }
    % Command \rowmidlinecw consists of 3 rules (top colour tablerowcolorodd, middle colour black, bottom colour white)
\newcommand{\rowmidlinecw}{\arrayrulecolor{tablerowcolorodd}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{white}\specialrule{\belowrulesep}{0pt}{0pt}%
         \arrayrulecolor{black}}
    % Command \rowmidlinewc consists of 3 rules (top colour white, middle colour black, bottom colour tablerowcolorodd)
\newcommand{\rowmidlinewc}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\lightrulewidth}{0pt}{0pt}%
            \arrayrulecolor{tablerowcolorodd}\specialrule{\belowrulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinew consists of 1 white rule
\newcommand{\rowmidlinew}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \rowmidlinec consists of 1 tablerowcolorodd rule
\newcommand{\rowmidlinec}{\arrayrulecolor{tablerowcolorodd}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}}
    % Command \bottomline consists of 2 rules (top colour
\newcommand{\bottomline}{\arrayrulecolor{white}\specialrule{\aboverulesep}{0pt}{-2pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%
\newcommand{\bottomlinec}{\arrayrulecolor{tablerowcolorodd}\specialrule{\aboverulesep}{0pt}{0pt}%
            \arrayrulecolor{black}\specialrule{\heavyrulewidth}{0pt}{\belowbottomsep}}%










\setbeamercovered{highly dynamic}


\author{Louis Onrust}
\title{$p(\text{conclusions} | \text{Skipping \{*2*\}})$}
\subtitle{Bayesian Language Modelling with Skipgrams}
\date{}
\begin{document}

\begin{frame}
    \titlepage
\end{frame}
\note[itemize]{
}

\begin{frame}{Bayesian Language Modelling with Skipgrams}
    \begin{block}{}
        Louis Onrust \\
        Centre for Language Studies, Radboud University \\
        Center for Processing Speech and Images, KU Leuven
    \end{block}

    \begin{block}{}
        \href{mailto:l.onrust@let.ru.nl}{l.onrust@let.ru.nl} \\
        \href{https://github.com/naiaden}{github.com/naiaden}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Scope of the Project}
    \begin{block}{Scope}
        \begin{itemize}
            \item Language models
            \item Latent variable models
            \item Domain-dependence of LVLM
            \item Intrinsic \& extrinsic evaluation
        \end{itemize}
    \end{block}

    \begin{block}{Goal}
        \begin{itemize}
            \item Bring back language modelling in Bayesian language modelling
            \item Improve cross domain langauge modelling with skipgrams
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Language Model}
    \begin{block}{Traditional method}
        The process:
        \begin{itemize}
            \item Read $n$-gram $p$
            \item Increment frequency of $p$
            \item Repeat, preferably ad infinitum
        \end{itemize}

        $n$-gram probabilities are then determined by their MLE
    \end{block}

  \uncover<2>{
    \begin{block}{Smoothed Traditional Language Model}
        What to do when the occurrence count of $p$ is 0?
        \begin{itemize}
            \item Not assign 0 as probability $\rightarrow$ smoothing
            \item Fall back to the last $(n-1)$ words of $p$ $\rightarrow$ backoff
        \end{itemize}

        One of the best methods is still Modified Kneser-Ney: backoff and smoothing
    \end{block}
  }
\end{frame}
\note[itemize]{
}

\begin{frame}{Language Model}
    \begin{block}{Bayesian method}
        \begin{itemize}
            \item Assume texts are generated by some process
            \item Consider the texts to be a sample from the process
            \item Infer underlying process
        \end{itemize}
    \end{block}

  \uncover<2->{
    \begin{block}{Bayesian Unigram Language Model: Chinese Restaurant Process}
        \begin{itemize}
            \item Clusters are tables, unigram tokens are customers
            \item Initially tokens seat at the same type table
            \item In the inference step, customers get to choose a new identity
        \end{itemize}
    \end{block}
  }
  \uncover<3->{
    \begin{block}{Bayesian $n$-gram Language Model: Nested Chinese Restaurant Process}
        \begin{itemize}
            \item Each context is a restaurant
            \item Each $n$ is a floor
            \item Each $n$-gram is a table
            \item Each $(n-1)$-gram sits at a table on the $(n-1)$th floor
            \item All restaurants share the same global menu
        \end{itemize}
    \end{block}
  }
\end{frame}
\note[itemize]{
    \note word types/tables: 1625150 1.6M
    \note word tokens/guests: 70310246 70M
    \note n=4: 31044735 CRPs 31M
    \note n=3: 11624400 CRPs 12M
    \note n=2: 1447377 CRPs 1.5M
    \note n=1: 1 CRP
    \note 56h, 100iterations
}

\begin{frame}{Bayesian Language Model: Learning \& Estimating}
    \begin{block}{Chinese Restaurant Process: Empirical Distribution}
        \begin{itemize}
            \item Each $n$-gram enters the restaurant, and goes to the $n$th floor, to the room that represents the context
            \item There he seeks for the table with other $n$-grams of the same type
                \begin{itemize}
                    \item If there is such a table, he joins that table
                    \item Otherwise he seats himself at an empty table
                \end{itemize}
            \item For each new table, a family member of the same $n$-gram but of length $(n-1)$ is sent to represent the family
                \begin{itemize}
                    \item This process repeats for $0 < x \leq n$
                \end{itemize}
        \end{itemize}
    \end{block}

  \uncover<2>{
    \begin{block}{Chinese Restaurant Process: Inference}
        With $m$ customers in the restaurant, a customer re-enters the restaurant and sits a table $t$ with probability
        \begin{itemize}
            \item $\frac{1}{m+1}$ with another $n$-gram $p$, or $\frac{|t|}{m+1}$ at the same table as $p$
            \item $\frac{1}{m+1}$ at a new table
        \end{itemize}
        The number of tables grows logarithmically
    \end{block}
  }
\end{frame}

\begin{frame}{Processes and Priors}
    \begin{block}{\only<1-2>{The Generative Model}\only<3>{A Suboptimal Unigram Language Model}}
        We described a Chinese restaurant process mixture model
        \only<1-2>{
          \begin{align}
          \pi_{[M]} &\sim \operatorname{CRP}(M)\label{eq:CRPMMpartition} \\
          \phi_t | \pi_{[M]} &\sim G_0 && \text{ for }t\in\pi_{[M]},\label{eq:CRPMMlatent} \\
          x_i|\phi,\pi_{[M]} &\sim F(\phi_t) && \text{ for }t\in\pi_{[M]}\text{ and }i\in t\label{eq:CRPMMdatapoints}
          \end{align}
        }
        \only<3>{
            \begin{align}
                G_0 &= \mathcal{U} \\
                G &\sim \operatorname{CRP}(G_0) \\
                x_i &\sim G
            \end{align}
        }
    \end{block}

    \only<1-2>{\uncover<2>{
        \begin{block}{Nested Pitman-Yor Chinese Restaurant Process}
            \begin{itemize}
                \item CRP and DPCRP give logarithmic growth
                \item Language manifests typically in power law growth
                \item PYCRP as generalisation of CRP and DPCRP
                    \begin{description}
                        \item[CRP] No parameters
                        \item[DPCRP] Concentration parameter $\alpha$
                        \item[PYCRP] Concentration parameter $\alpha$ and discount parameter $\gamma$
                    \end{description}
            \end{itemize}
        \end{block}
    }}
    \only<3>{
        \begin{block}{Nested Pitman-Yor Chinese Restaurant Process Mixture Model}
            \begin{align}
                G_0 &= \mathcal{U} \\
                G_1 &\sim\operatorname{PYCRP}(\alpha_1,\gamma_1,G_0)\\
                G_u &\sim\operatorname{PYCRP}(\alpha_{|u|},\gamma_{|u|},G_{\pi(u)})\\                
                x_i|u_j &\sim G_{u_j}
            \end{align}
        \end{block}
    }
\end{frame}
\note[itemize]{
}

\begin{frame}{Bayesian Language Model: The Implementation}
    \begin{block}{Motivation}
        Existing Bayesian language models\ldots
        \begin{itemize}
            \item are merely an algorithmic showcase without real language modelling aspirations
            \item cannot handle really big data sets
        \end{itemize}
    \end{block}

  \uncover<2>{
    \begin{block}{Implementation}
        We use the following software:
        \begin{description}
            \item[cpyp] an existing \CC{} framework on BNP with PYP priors
            \item[colibri] an existing \CC{} framework for pattern modelling
        \end{description}
    \end{block}

    \begin{block}{Advantages}
        \begin{itemize}
            \item We can now handle many patterns such as $n$-grams, skipgrams, and flexgrams
            \item Tresholding patterns on many levels
            %\item Efficient storage of patterns
        \end{itemize}
    \end{block}
  }
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: The Setup}
    \begin{block}{Data Sets}
        \begin{itemize}
            \item JRC-Acquis English
            \item Google 1 billion words
            \item EMEA English
            \item Wikipedia English
        \end{itemize}
    \end{block}

  \uncover<2->{
    \begin{block}{Backoff Methods}
        \begin{description}
            \item[ngram] full recursive backoff to shorter $n$-grams
            \item[limited] recursive backoff to all patterns $\leq n$ until match
            \item[full] recursive backoff to all patterns $\leq n$
        \end{description}
    \end{block}
  }
  \uncover<3>{
    \begin{block}{Evaluation Measure}
        \begin{itemize}
            \item Intrinsic evaluation with perplexity
        \end{itemize}
    \end{block}
  }
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: An Overview}
    \begin{block}{Summary}
        \begin{itemize}
            \item Within-domain evaluation yields best performance
            \item Adding skipgrams increases performance on cross-domain evaluation
            \item For generic corpora, limited recursive backoff performs best
            \item Seems to outperform Generalised Language Model
            \item If significant, perhaps not enough for extrinsic evaluation
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: Domains and Patterns}

    \begin{block}{Observations}
        \begin{description}
            \item[domains] Within-domain evaluation yields best performance
            \item[patterns] Adding skipgrams increases performance on cross-domain evaluation
        \end{description}
    \end{block}
    %\vspace{-1em}
    {\small
    \begin{tabular}{rrrrrrrrrr} 
    & \multicolumn{4}{c}{\cellcolor{ruhuisstijlrood}\color{white} $n$-gram} & & \multicolumn{4}{c}{\cellcolor{ruhuisstijlrood}\color{white} skipgram} \\
    \headcol & {\color{white} jrc} & {\color{white} 1bw} & {\color{white} emea} & {\color{white} wp} & & {\color{white} jrc} & {\color{white} 1bw} & {\color{white} emea} & {\color{white} wp}\\
    \rowcolodd jrc & 13  & 1195 & 961 & 1011 & & 13  & {\color{ruhuisstijlrood} 1162} & {\color{ruhuisstijlrood} 939} & {\color{ruhuisstijlrood} 1008}\\
    \rowcoleven 1bws & 768 & {\color{ruhuisstijlrood} 158}  & 945 & {\color{ruhuisstijlrood} 493} & & {\color{ruhuisstijlrood} 751} & 162  & {\color{ruhuisstijlrood} 921} & 507\\
    \rowcolodd emea& 600 & {\color{ruhuisstijlrood} 1143} & 4   & 843 & & {\color{ruhuisstijlrood} 581} & 1155 & 4   & {\color{ruhuisstijlrood} 842}\\
    \rowcoleven wps  & {\color{ruhuisstijlrood} 555} & {\color{ruhuisstijlrood} 455} & 1005 & {\color{ruhuisstijlrood} 217} & & 565 & 470  & {\color{ruhuisstijlrood} 990} & 227\\
    \end{tabular}
    }
\end{frame}
\note[itemize]{
}

\begin{frame}{Results: Effect of Different Backoff Methods}
    \begin{block}{Observations}
        \begin{description}
            \item[backoff] For generic corpora, limited recursive backoff performs best
        \end{description}
    \end{block}
    %\vspace{-1em}
    {\small
        \begin{tabular}{rrrrrrrrrrr}
        & & \multicolumn{4}{c}{\cellcolor{ruhuisstijlrood}\color{white} $n$-gram} & & \multicolumn{4}{c}{\cellcolor{ruhuisstijlrood}\color{white} skipgram} \\
\headcol %\rowfont{\color{white}}
 &  & {\color{white} jrc} & {\color{white} 1bw} & {\color{white} emea} & {\color{white} wp} &  & {\color{white} jrc} & {\color{white} 1bw} & {\color{white} emea} & {\color{white} wp} \\
\rowcolodd & ngram & {\color{ruhuisstijlrood} 13} & 1510 & 1081 & 1293 &  & {\color{ruhuisstijlrood} 13} & 1843 & 1295 & 1623 \\
\rowcolodd & limited & 14 & 1477 & 1122 & 1263 &  & {\color{ruhuisstijlrood} 13} & 1542 & 1149 & 1356 \\ 
\rowcolodd \multirow{-3}{*}{jrc} & full & 69 & {\color{ruhuisstijlrood} 1195} & {\color{ruhuisstijlrood} 961} & {\color{ruhuisstijlrood} 1011} &  & 65 & {\color{ruhuisstijlrood}\bf 1162} & {\color{ruhuisstijlrood}\bf 939} & {\color{ruhuisstijlrood}\bf 1008} \\
\rowcoleven & ngram & {\color{ruhuisstijlrood} 768} & {\color{ruhuisstijlrood}\bf 158} & {\color{ruhuisstijlrood} 946} & {\color{ruhuisstijlrood}\bf 493} &  & 879 & 163 & 1105 & 550 \\ 
\rowcoleven & limited & 815 & 185 & 1025 & 563 &  & {\color{ruhuisstijlrood}\bf 751} & {\color{ruhuisstijlrood} 162} & {\color{ruhuisstijlrood}\bf 921} & {\color{ruhuisstijlrood} 507} \\
\rowcoleven \multirow{-3}{*}{1bws} & full & 800 & 264 & 1039 & 583 &  & 769 & 252 & 988 & 561 \\ 
\rowcolodd & ngram & 769 & 1552 & {\color{ruhuisstijlrood} 4} & 1097 &  & 969 & 2090 & {\color{ruhuisstijlrood} 4} & 1416 \\ 
\rowcolodd & limited & 779 & 1385 & {\color{ruhuisstijlrood} 4} & 1018 &  & 838 & 1655 & {\color{ruhuisstijlrood} 4} & 1139 \\ 
\rowcolodd \multirow{-3}{*}{emea} & full & {\color{ruhuisstijlrood} 600} & {\color{ruhuisstijlrood}\bf 1143} & 32 & {\color{ruhuisstijlrood} 843} &  & {\color{ruhuisstijlrood}\bf 581} & {\color{ruhuisstijlrood} 1155} & 32 & {\color{ruhuisstijlrood}\bf 842} \\  
\rowcoleven & ngram & {\color{ruhuisstijlrood}\bf 555} & {\color{ruhuisstijlrood}\bf 455} & {\color{ruhuisstijlrood} 1005} & {\color{ruhuisstijlrood}\bf 217} &  & 623 & 504 & 1,132 & 233 \\ 
\rowcoleven & limited & 629 & 543 & 1168 & 260 &  & {\color{ruhuisstijlrood} 565} & {\color{ruhuisstijlrood} 470} & {\color{ruhuisstijlrood}\bf 990} & {\color{ruhuisstijlrood} 227} \\ 
\rowcoleven \multirow{-3}{*}{wps} & full & 656 & 579 & 1184 & 357 &  & 625 & 548 & 1,106 & 336 \\
\end{tabular}
    }
\end{frame}
\note[itemize]{
}

\begin{frame}{Future Work}
    \begin{block}{Experiments}
        \begin{itemize}
            \item Validate significance by testing on multiple languages
            \item Investigate influence skipgrams with qualitative analysis
            \item When we find a more substantial drop in perplexity:
                \begin{itemize}
                    \item Machine translation experiments
                    \item Automated speech recognition experiments
                \end{itemize}
            \item Investigate multi-domain language models (DHPYPLM)
            \item Generalise skipgrams to flexgrams
            \item \ldots
        \end{itemize}
    \end{block}
\end{frame}
\note[itemize]{
}

\end{document}

